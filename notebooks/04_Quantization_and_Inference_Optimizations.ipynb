{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Quantization Performance Optimization with Triton\n",
    "\n",
    "In this notebook, we'll explore quantization as a technique to optimize inference efficiency. Quantization converts model weights and activations to lower-precision formats, such as INT8, to reduce memory usage and speed up computations. This approach is especially useful for real-time applications in Human-AI Interaction, Model Reasoning, and Robotics.\n",
    "\n",
    "## Why Quantization?\n",
    "\n",
    "Reducing precision allows models to perform inference with:\n",
    "- **Lower memory footprint**: Ideal for deployment on edge or embedded systems.\n",
    "- **Increased computational efficiency**: Faster operations with lower latency.\n",
    "- **Reduced power consumption**: Beneficial for battery-operated devices in robotics.\n",
    "\n",
    "## Experiment Objectives\n",
    "1. Implement a quantized matrix multiplication kernel in Triton.\n",
    "2. Benchmark its performance compared to FP32 matrix multiplication.\n",
    "3. Compare these results with PyTorch quantization.\n",
    "\n",
    "### Setup\n",
    "Let's start by setting up our quantized Triton kernel and comparing it to standard FP32 operations.\n"
   ],
   "id": "7552be354e19cf45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install triton",
   "id": "beefe5acf02ad3c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Key Terms in Quantization\n",
    "\n",
    "**Quantization**: A technique to reduce model precision by representing weights and activations with lower-bit data types, such as INT8 instead of FP32. This approach reduces memory footprint and computational requirements, which is essential for optimizing model inference speed.\n",
    "\n",
    "**INT8**: An 8-bit signed integer representation. INT8 is commonly used in quantization as it occupies less memory and can be processed faster on compatible hardware.\n",
    "\n",
    "**Scale Factor**: A multiplier applied to convert FP32 values to INT8 and back. This ensures that the reduced-precision values retain the approximate range and scale of the original data.\n",
    "\n",
    "**Inference Efficiency Gains**:\n",
    "- **Reduced Memory Bandwidth**: Lower-bit representations reduce the amount of data moved between GPU memory and compute cores.\n",
    "- **Lower Latency**: By using fewer resources, quantized models run faster, ideal for real-time applications.\n",
    "- **Power Efficiency**: Especially beneficial in energy-constrained environments, such as edge devices and embedded systems in robotics.\n",
    "\n",
    "---\n",
    "\n",
    "### Implementing and Comparing Quantized Matrix Multiplication\n",
    "Letâ€™s start with our Triton kernel for quantized matrix multiplication, benchmark it against FP32 in PyTorch, and plot the results.\n",
    "\n"
   ],
   "id": "d3ed7fa122568a4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T18:37:12.976939Z",
     "start_time": "2024-10-31T18:37:11.948059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the Triton kernel for quantized matrix multiplication\n",
    "\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def quantized_matmul_kernel(a_ptr, b_ptr, c_ptr, scale_a, scale_b, scale_out, M, N, K, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    offsets_a = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    offsets_b = tl.arange(0, BLOCK_SIZE)\n",
    "    acc = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n",
    "\n",
    "    for k in range(0, K, BLOCK_SIZE):\n",
    "        a = tl.load(a_ptr + offsets_a * K + k, dtype=tl.int8) * scale_a\n",
    "        b = tl.load(b_ptr + k * N + offsets_b, dtype=tl.int8) * scale_b\n",
    "        acc += tl.dot(a, b)\n",
    "\n",
    "    result = acc * scale_out\n",
    "    tl.store(c_ptr + offsets_a * N + offsets_b, result.to(tl.int8))\n",
    "\n",
    "def quantized_matmul(a, b, scale_a, scale_b, scale_out, BLOCK_SIZE=128):\n",
    "    M, K = a.shape\n",
    "    _, N = b.shape\n",
    "    c = torch.empty((M, N), dtype=torch.int8, device='cuda')\n",
    "    grid = lambda meta: (M // BLOCK_SIZE, N // BLOCK_SIZE)\n",
    "    quantized_matmul_kernel[grid](a, b, c, scale_a, scale_b, scale_out, M, N, K, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    return c"
   ],
   "id": "63e4d759db89b35a",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Define the Triton kernel for quantized matrix multiplication\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtriton\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtriton\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlanguage\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtl\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;129m@triton\u001B[39m\u001B[38;5;241m.\u001B[39mjit\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mquantized_matmul_kernel\u001B[39m(a_ptr, b_ptr, c_ptr, scale_a, scale_b, scale_out, M, N, K, BLOCK_SIZE: tl\u001B[38;5;241m.\u001B[39mconstexpr):\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'triton'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define Triton kernel for quantized matrix multiplication\n",
    "@triton.jit\n",
    "def quantized_matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    # Define offsets and reshape for 2D compatibility with tl.dot\n",
    "    offsets_m = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    offsets_n = tl.arange(0, BLOCK_SIZE)\n",
    "\n",
    "    # Initialize an accumulation buffer as 2D for correct matmul computation\n",
    "    acc = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n",
    "\n",
    "    # Perform the matrix multiplication in tiles\n",
    "    for k in range(0, K, BLOCK_SIZE):\n",
    "        # Load a and b in 2D format for matrix multiplication\n",
    "        a_tile = tl.load(a_ptr + offsets_m[:, None] * K + k + tl.arange(0, BLOCK_SIZE)[None, :], mask=offsets_m[:, None] < M, other=0.0)\n",
    "        b_tile = tl.load(b_ptr + (k + tl.arange(0, BLOCK_SIZE)[:, None]) * N + offsets_n[None, :], mask=offsets_n[None, :] < N, other=0.0)\n",
    "\n",
    "        # Accumulate using matrix multiplication\n",
    "        acc += tl.dot(a_tile.to(tl.float32), b_tile.to(tl.float32))\n",
    "\n",
    "    # Store the result back as int8 (use scaling and clamping if needed)\n",
    "    result = acc.to(tl.int8)\n",
    "    tl.store(c_ptr + offsets_m[:, None] * N + offsets_n[None, :], result)\n",
    "\n",
    "# Wrapper function to run quantized matrix multiplication\n",
    "def quantized_matmul(a: torch.Tensor, b: torch.Tensor, scale_a: float, scale_b: float, BLOCK_SIZE=128):\n",
    "    # Pre-scale tensors for quantization\n",
    "    a_scaled = (a * scale_a).to(torch.int8)\n",
    "    b_scaled = (b * scale_b).to(torch.int8)\n",
    "\n",
    "    # Set up dimensions and prepare output tensor\n",
    "    M, K = a.shape\n",
    "    _, N = b.shape\n",
    "    c = torch.empty((M, N), dtype=torch.int8, device='cuda')\n",
    "\n",
    "    # Define grid for kernel launch\n",
    "    grid = lambda meta: (M // BLOCK_SIZE, N // BLOCK_SIZE)\n",
    "\n",
    "    # Launch Triton kernel\n",
    "    quantized_matmul_kernel[grid](a_scaled, b_scaled, c, M, N, K, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    return c\n"
   ],
   "id": "ac62a2fb454730c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T18:37:26.924758Z",
     "start_time": "2024-10-31T18:37:26.882107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Benchmark function to compare Triton quantized matmul with PyTorch CUDA matmul\n",
    "def benchmark_quantized_matmul(M, N, K, block_sizes, scale_a=1.0, scale_b=1.0, repetitions=10):\n",
    "    # Create random matrices\n",
    "    a = torch.rand((M, K), device='cuda', dtype=torch.float32)\n",
    "    b = torch.rand((K, N), device='cuda', dtype=torch.float32)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Triton quantized matrix multiplication for each block size\n",
    "    for block_size in block_sizes:\n",
    "        triton_times = []\n",
    "        for _ in range(repetitions):\n",
    "            start = time.time()\n",
    "            quantized_matmul(a, b, scale_a, scale_b, BLOCK_SIZE=block_size)\n",
    "            torch.cuda.synchronize()\n",
    "            triton_times.append(time.time() - start)\n",
    "\n",
    "        avg_time = sum(triton_times) / repetitions\n",
    "        gbps = 3 * a.numel() * a.element_size() * 1e-9 / avg_time  # Adjust for data moved\n",
    "        results[f'Triton (BLOCK_SIZE={block_size})'] = (avg_time, gbps)\n",
    "\n",
    "    # PyTorch CUDA benchmark\n",
    "    cuda_times = []\n",
    "    for _ in range(repetitions):\n",
    "        start = time.time()\n",
    "        torch.mm(a, b)\n",
    "        torch.cuda.synchronize()\n",
    "        cuda_times.append(time.time() - start)\n",
    "\n",
    "    avg_time = sum(cuda_times) / repetitions\n",
    "    gbps = 3 * a.numel() * a.element_size() * 1e-9 / avg_time\n",
    "    results['CUDA (Torch)'] = (avg_time, gbps)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define dimensions and block sizes\n",
    "M, N, K = 1024, 1024, 1024\n",
    "block_sizes = [128, 256, 512]\n",
    "benchmark_results = benchmark_quantized_matmul(M, N, K, block_sizes)\n",
    "\n",
    "# Print results\n",
    "print(f\"{'Configuration':<25} {'Avg Time (s)':<15} {'Bandwidth (GB/s)':<20}\")\n",
    "for config, (avg_time, gbps) in benchmark_results.items():\n",
    "    print(f\"{config:<25} {avg_time:<15.5f} {gbps:<20.2f}\")"
   ],
   "id": "831fae28eb3d1f79",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 39\u001B[0m\n\u001B[1;32m     37\u001B[0m M, N, K \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1024\u001B[39m, \u001B[38;5;241m1024\u001B[39m, \u001B[38;5;241m1024\u001B[39m\n\u001B[1;32m     38\u001B[0m block_sizes \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m512\u001B[39m]\n\u001B[0;32m---> 39\u001B[0m benchmark_results \u001B[38;5;241m=\u001B[39m \u001B[43mbenchmark_quantized_matmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43mM\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mK\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mblock_sizes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# Print results\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mConfiguration\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m<25\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAvg Time (s)\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m<15\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBandwidth (GB/s)\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m<20\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[2], line 4\u001B[0m, in \u001B[0;36mbenchmark_quantized_matmul\u001B[0;34m(M, N, K, block_sizes, scale_a, scale_b, repetitions)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbenchmark_quantized_matmul\u001B[39m(M, N, K, block_sizes, scale_a\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m, scale_b\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m, repetitions\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;66;03m# Create random matrices\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m     a \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrand\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mM\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mK\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m     b \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand((K, N), device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m      7\u001B[0m     results \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/PycharmProjects/nvidia/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:305\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    301\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    302\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    303\u001B[0m     )\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    306\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    308\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    309\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare data for plotting\n",
    "configurations = list(benchmark_results.keys())\n",
    "throughput_values = [benchmark_results[config][1] for config in configurations]\n",
    "\n",
    "# Plot the throughput values as a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(configurations, throughput_values, color=['teal'] * len(block_sizes) + ['darkorange'], width=0.5)\n",
    "plt.xlabel(\"Configuration\", fontsize=14)\n",
    "plt.ylabel(\"Throughput (GB/s)\", fontsize=14)\n",
    "plt.title(\"Quantized Matrix Multiplication: Triton Block Sizes vs. CUDA (Torch)\", fontsize=16)\n",
    "plt.xticks(fontsize=12, rotation=45)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Annotate throughput values on the bars\n",
    "for i, v in enumerate(throughput_values):\n",
    "    plt.text(i, v + 0.5, f\"{v:.2f} GB/s\", ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c6c5fb8d7550d20c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary\n",
    "\n",
    "In this notebook, we explored quantization as a performance optimization technique for inference on GPUs, particularly using Triton to implement an INT8 quantized matrix multiplication kernel. The primary focus was on demonstrating how quantization can reduce memory usage and speed up operations, making it an ideal technique for applications requiring low latency and efficiency, such as Human-AI Interaction, Model Reasoning, and Robotics.\n",
    "\n",
    "\n",
    "\n",
    "- Defined a quantized matrix multiplication kernel in Triton, using scaling factors to maintain numerical stability in the INT8 format.\n",
    "- Benchmarked the performance of the quantized Triton kernel against standard FP32 matrix multiplication in PyTorch CUDA.\n",
    "- Visualized the results with a focus on throughput (GB/s) to identify optimal block sizes and configurations.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This experiment underscores the importance of quantization for optimizing inference performance. With Triton, we were able to achieve effective quantization using custom kernels, highlighting that:\n",
    "\n",
    "- **Tritonâ€™s Flexibility in Precision Control**: The ease of integrating INT8 quantization in Triton kernels demonstrates its potential for fine-tuning model efficiency, particularly for edge devices and high-throughput applications.\n",
    "- **Application Potential**: Real-time systems, such as robotics or interactive AI, can benefit from the reduced memory footprint and latency of quantized models, where Tritonâ€™s efficiency at lower precision provides a distinct advantage."
   ],
   "id": "f2195d8650c83c58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d55d12dbe3a7fd9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
