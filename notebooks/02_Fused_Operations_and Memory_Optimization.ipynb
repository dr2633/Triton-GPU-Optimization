{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fused Operations and Memory Optimization\n",
    "\n",
    "In this notebook, we’ll explore the concept of **kernel fusion** and how it optimizes memory usage and computational efficiency on GPUs. Kernel fusion combines multiple operations within a single kernel to reduce memory overhead, improve memory bandwidth utilization, and minimize data transfer between memory and the GPU.\n",
    "\n",
    "## Why Kernel Fusion?\n",
    "\n",
    "When performing multiple operations on the same data, GPUs typically move data back and forth between memory and compute cores for each operation. This process can create memory bottlenecks and limit the overall throughput of the GPU. **Kernel fusion** addresses this issue by:\n",
    "- Reducing the number of memory accesses.\n",
    "- Enabling operations to be computed within a single pass, thereby lowering latency.\n",
    "- Optimizing usage of memory bandwidth and caches.\n",
    "\n",
    "**Example Use Cases**:\n",
    "- **Add-Multiply Operation**: A common scenario in deep learning where values are summed and then multiplied by a constant or another vector.\n",
    "- **Batch Normalization**: Several operations (mean, variance, scale, shift) can be fused into a single kernel.\n",
    "- **Convolutional Layers**: Various activation functions and scaling operations can be fused to improve efficiency.\n",
    "\n",
    "In this notebook, we’ll:\n",
    "1. Implement a fused **add-multiply** operation in Triton.\n",
    "2. Benchmark the performance of the fused operation across different block sizes.\n",
    "3. Compare the performance of our fused operation with a similar PyTorch (CUDA) implementation.\n",
    "\n",
    "---\n",
    "\n",
    "## Setting Up the Fused Add-Multiply Kernel\n",
    "\n",
    "To get started, let’s write a Triton kernel that performs an elementwise addition of two vectors, followed by multiplication with a scalar, all in a single kernel. This will demonstrate how fusion reduces the need for multiple memory transfers.\n",
    "\n"
   ],
   "id": "e0efc7d5f0a6e58b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install triton",
   "id": "8120c21fb419612e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU found. Please enable GPU under Runtime > Change runtime type.\")"
   ],
   "id": "2b747663dab98352"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implementing the Fused Add-Multiply Kernel\n",
    "\n",
    "To start, let’s write a Triton kernel that performs an elementwise addition of two vectors, followed by multiplication with a scalar, all in a single kernel. This will demonstrate how fusion reduces the need for multiple memory transfers.\n",
    "\n",
    "#### Fused Add-Multiply Kernel in Triton\n",
    "\n",
    "Run the cell below to implement our fused kernel in Triton."
   ],
   "id": "1a5b16fe75d98e17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T17:43:40.467877Z",
     "start_time": "2024-10-30T17:43:39.533136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# Set a fixed random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Triton kernel for fused addition and multiplication\n",
    "@triton.jit\n",
    "def fused_add_mul_kernel(x_ptr, y_ptr, output_ptr, scalar, n_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    # Identify program ID for each block in the 1D grid\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    # Calculate starting position for each thread block\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "\n",
    "    # Mask to ensure we don’t access out-of-bounds memory\n",
    "    mask = offsets < n_elements\n",
    "\n",
    "    # Load elements from x and y using the mask\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "\n",
    "    # Perform the fused addition and multiplication operation\n",
    "    output = (x + y) * scalar\n",
    "\n",
    "    # Store the result in output, using the mask for bounds safety\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n"
   ],
   "id": "83f2950bafcb9c91",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtriton\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtriton\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlanguage\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtl\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Set a fixed random seed for reproducibility\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'triton'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Running the Fused Kernel\n",
    "\n",
    "Next, we’ll create a function to call our fused kernel with different `BLOCK_SIZE` values and benchmark its performance. We’ll compare this to a PyTorch operation that performs the same add-multiply calculation in separate steps."
   ],
   "id": "b3200c1ea207998c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fused Add- Multiply Function and Benchmarking\n",
    "\n",
    "import time\n",
    "\n",
    "# Function to run the fused add-multiply operation in Triton\n",
    "def fused_add_mul(x: torch.Tensor, y: torch.Tensor, scalar=2.0, BLOCK_SIZE=128):\n",
    "    output = torch.empty_like(x)\n",
    "    n_elements = output.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "    fused_add_mul_kernel[grid](x, y, output, scalar, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    return output\n",
    "\n",
    "# PyTorch CUDA baseline function\n",
    "def add_multiply_cuda(x, y, scalar=2.0):\n",
    "    result = (x + y) * scalar\n",
    "    return result\n",
    "\n",
    "# Benchmark function to compare Triton and CUDA for fused operations\n",
    "def benchmark_fused_operations(size, block_sizes, scalar=2.0, repetitions=10):\n",
    "    x = torch.rand(size, device='cuda', dtype=torch.float32)\n",
    "    y = torch.rand(size, device='cuda', dtype=torch.float32)\n",
    "    results = {}\n",
    "\n",
    "    # Triton benchmarks for each block size\n",
    "    for block_size in block_sizes:\n",
    "        triton_times = []\n",
    "        for _ in range(repetitions):\n",
    "            start = time.time()\n",
    "            fused_add_mul(x, y, scalar, BLOCK_SIZE=block_size)\n",
    "            torch.cuda.synchronize()\n",
    "            triton_times.append(time.time() - start)\n",
    "\n",
    "        avg_time = sum(triton_times) / repetitions\n",
    "        gbps = 3 * x.numel() * x.element_size() * 1e-9 / avg_time\n",
    "        results[f'Triton (BLOCK_SIZE={block_size})'] = (avg_time, gbps)\n",
    "\n",
    "    # PyTorch CUDA benchmark\n",
    "    cuda_times = []\n",
    "    for _ in range(repetitions):\n",
    "        start = time.time()\n",
    "        add_multiply_cuda(x, y, scalar)\n",
    "        torch.cuda.synchronize()\n",
    "        cuda_times.append(time.time() - start)\n",
    "\n",
    "    avg_time = sum(cuda_times) / repetitions\n",
    "    gbps = 3 * x.numel() * x.element_size() * 1e-9 / avg_time\n",
    "    results['CUDA (Torch)'] = (avg_time, gbps)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define dimensions and block sizes\n",
    "size = 1024 * 1024\n",
    "block_sizes = [128, 256, 512, 1024]\n",
    "benchmark_results = benchmark_fused_operations(size, block_sizes)\n",
    "\n",
    "# Print results in a table format\n",
    "print(f\"{'Configuration':<25} {'Avg Time (s)':<15} {'Bandwidth (GB/s)':<20}\")\n",
    "for config, (avg_time, gbps) in benchmark_results.items():\n",
    "    print(f\"{config:<25} {avg_time:<15.5f} {gbps:<20.2f}\")\n"
   ],
   "id": "66124607c956995c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualize these results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for plotting\n",
    "configurations = list(benchmark_results.keys())\n",
    "throughput_values = [benchmark_results[config][1] for config in configurations]\n",
    "\n",
    "# Plot the throughput values as a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(configurations, throughput_values, color=['teal'] * len(block_sizes) + ['darkorange'], width=0.5)\n",
    "plt.xlabel(\"Configuration\", fontsize=14)\n",
    "plt.ylabel(\"Throughput (GB/s)\", fontsize=14)\n",
    "plt.title(\"Fused Add-Multiply Operation: Triton Block Sizes vs. CUDA (Torch)\", fontsize=16)\n",
    "plt.xticks(fontsize=12, rotation=45)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Annotate throughput values on the bars\n",
    "for i, v in enumerate(throughput_values):\n",
    "    plt.text(i, v + 0.5, f\"{v:.2f} GB/s\", ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "23fa543a4b14c975"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Analysis of Results\n",
    "\n",
    "The performance results demonstrate that **block sizes of 256 and 512 provide the highest throughput** for the fused add-multiply operation on the NVIDIA T4 GPU, whereas **block size 128 performs less optimally**. Let’s analyze the reasons behind these findings:\n",
    "\n",
    "#### **1. Thread Occupancy and GPU Utilization**\n",
    "- **Block Size 128**: The smaller block size may result in underutilization of the GPU’s processing resources. On GPUs like the T4, which have multiple streaming multiprocessors (SMs), smaller block sizes may lead to fewer active threads, resulting in lower occupancy and suboptimal utilization of available GPU cores.\n",
    "- **Block Sizes 256 and 512**: These sizes typically balance workload across the SMs, maximizing thread occupancy and core utilization. With more threads running concurrently, latency from memory accesses can be hidden more effectively, leading to higher performance.\n",
    "\n",
    "#### **2. Memory Access Patterns and Bandwidth Utilization**\n",
    "- With **fused operations**, memory access efficiency is crucial since we aim to minimize data movement and maximize cache usage. Larger blocks can better align with cache line sizes, reducing the need for frequent memory accesses.\n",
    "- At **256 and 512 block sizes**, each block accesses memory more efficiently, leading to better cache utilization and fewer memory bank conflicts. This configuration allows for better memory bandwidth utilization, resulting in the higher throughput observed.\n",
    "\n",
    "#### **3. Optimal Thread Scheduling**\n",
    "- GPUs use a scheduler to swap active warps (groups of threads) in and out of execution based on memory availability. With **block sizes 256 and 512**, more threads are available per SM, allowing the scheduler to optimize active thread usage and reduce idle time.\n",
    "- **Block size 128** likely results in more scheduling inefficiencies on the T4, as fewer threads per SM reduce the GPU’s ability to keep enough threads active to hide latency effectively.\n",
    "\n",
    "#### **4. Experiment Specifics**\n",
    "- In this experiment, the fused add-multiply operation benefits from a **balance between compute workload and memory efficiency**. While small block sizes like 128 allow for more granular thread control, they may also result in an increase in idle time and suboptimal cache utilization on T4 GPUs.\n",
    "- **Block sizes of 256 and 512** provide an ideal balance between computational load per block and memory access efficiency, explaining their superior throughput.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion and Summary\n",
    "\n",
    "In this notebook, we have demonstrated how **kernel fusion**, implemented in Triton, can enhance memory and computational efficiency by combining multiple operations. From our performance analysis:\n",
    "- **Block Size 256 and 512 configurations yielded the highest throughput**, highlighting the importance of selecting an optimal block size based on GPU architecture and workload characteristics.\n",
    "- **Block Size 128**, although beneficial in some contexts, did not perform as well in this scenario due to lower occupancy and less efficient cache utilization on the T4 GPU.\n",
    "\n",
    "This experiment underscores that **choosing the right block size is critical in maximizing GPU performance**. For inference and other high-performance applications, testing and tuning block size based on specific hardware, like the T4 GPU, can yield substantial performance gains.\n"
   ],
   "id": "dfb13f2d880ecffa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "69da4f43c8e5648d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
