{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd859c82b818fe77",
   "metadata": {},
   "source": [
    "# Matrix Multiplication with Blocking and Tiling\n",
    "\n",
    "In this notebook, we will implement **matrix multiplication** with **blocking** and **tiling** strategies in Triton to optimize performance. Blocking and tiling improve memory locality and computational efficiency, which are essential for high-performance GPU operations, especially on large matrices.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "When working with large matrices, memory and compute requirements can be significant. Blocking and tiling can help optimize these operations by improving data locality and parallelism, maximizing the effective use of GPU resources.\n",
    "\n",
    "### Key Terms\n",
    "- **Blocking**: A technique that splits large data into smaller blocks to fit into faster, smaller memory like shared memory on a GPU. Each block is processed independently, which can lead to better cache utilization and reduced memory access latency.\n",
    "  \n",
    "- **Tiling**: A strategy where the computation is split into tiles that fit into the GPU’s shared memory. Tiling allows the GPU to load each tile once into shared memory and reuse it for multiple computations, reducing memory bandwidth requirements and improving performance.\n",
    "\n",
    "These methods help reduce the number of memory accesses and allow the GPU to work on small, manageable pieces of data, which is critical for efficiency in large-scale computations.\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "1. **Implement matrix multiplication** using blocking and tiling in Triton.\n",
    "2. **Benchmark performance** against PyTorch’s CUDA matrix multiplication.\n",
    "3. **Analyze the impact of blocking and tiling** on GPU memory utilization and performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Setting Up the Triton Kernel for Matrix Multiplication\n",
    "\n",
    "We start by defining a Triton kernel for matrix multiplication. This kernel will load blocks of the input matrices into shared memory, perform the multiplication for each block, and accumulate the results in a final matrix. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0b2d9e142fa6b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:02:03.289206Z",
     "start_time": "2024-10-31T19:02:02.578786Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtl\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# Triton kernel for matrix multiplication with blocking and tiling\n",
    "@triton.jit\n",
    "def matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, BLOCK_SIZE: tl.constexpr):\n",
    "    # Program ID in the 2D grid\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "\n",
    "    # Define tile/block start points in matrix A and B\n",
    "    offsets_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    offsets_n = pid_n * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    # Initialize accumulator for the output block\n",
    "    acc = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n",
    "    \n",
    "    # Loop over tiles in the shared K dimension\n",
    "    for k in range(0, K, BLOCK_SIZE):\n",
    "        # Load tiles of A and B\n",
    "        a_tile = tl.load(a_ptr + offsets_m[:, None] * K + (k + tl.arange(0, BLOCK_SIZE)[None, :]), mask=(offsets_m[:, None] < M) & (k + tl.arange(0, BLOCK_SIZE)[None, :] < K), other=0.0)\n",
    "        b_tile = tl.load(b_ptr + (k + tl.arange(0, BLOCK_SIZE)[:, None]) * N + offsets_n[None, :], mask=(k + tl.arange(0, BLOCK_SIZE)[:, None] < K) & (offsets_n[None, :] < N), other=0.0)\n",
    "        \n",
    "        # Perform the matrix multiplication for the tile\n",
    "        acc += tl.dot(a_tile, b_tile)\n",
    "    \n",
    "    # Write the computed block to the output matrix\n",
    "    tl.store(c_ptr + offsets_m[:, None] * N + offsets_n[None, :], acc, mask=(offsets_m[:, None] < M) & (offsets_n[None, :] < N))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a9aa00278dc6a2",
   "metadata": {},
   "source": [
    "### Explanation of the Code\n",
    "- **Kernel Definition**: The `matmul_kernel` function is decorated with `@triton.jit` for Triton’s JIT compilation.\n",
    "- **Grid Setup**: Each GPU thread block computes one tile of the result matrix `C`.\n",
    "- **Tiling and Blocking**:\n",
    "  - `offsets_m` and `offsets_n` define the starting positions of each tile within matrices `A` and `B`.\n",
    "  - Each tile is loaded into shared memory using the `tl.load` operation, enabling reuse within the tile computation.\n",
    "- **Loop Over K Dimension**: The loop iterates over the shared dimension `K`, loading one tile at a time and accumulating the results in `acc`.\n",
    "- **Result Storage**: The result tile is stored back into `c_ptr`, which represents the output matrix.\n",
    "\n",
    "---\n",
    "\n",
    "## Running the Matrix Multiplication with Tiling\n",
    "\n",
    "Let's define a wrapper function to set up the input matrices and launch the Triton kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e9923a636fd5c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:02:20.504892Z",
     "start_time": "2024-10-31T19:02:20.502190Z"
    }
   },
   "outputs": [],
   "source": [
    "def matmul_tiled(a: torch.Tensor, b: torch.Tensor, BLOCK_SIZE=128):\n",
    "    # Matrix dimensions\n",
    "    M, K = a.shape\n",
    "    _, N = b.shape\n",
    "    c = torch.empty((M, N), device='cuda', dtype=torch.float32)\n",
    "    \n",
    "    # Define grid size for kernel launch\n",
    "    grid = (M // BLOCK_SIZE, N // BLOCK_SIZE)\n",
    "    \n",
    "    # Launch the Triton kernel\n",
    "    matmul_kernel[grid](a, b, c, M, N, K, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df25b48d02cbd30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Benchmarking the Tiled Matrix Multiplication\n",
    "\n",
    "Now that we have the tiled matrix multiplication kernel, let's benchmark it against PyTorch's CUDA matrix multiplication (`torch.mm`) to see how our blocking and tiling implementation performs.\n",
    "\n",
    "We’ll vary the block size to understand its impact on throughput and identify the optimal block size for our setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4251f3479b5dc17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:02:46.236952Z",
     "start_time": "2024-10-31T19:02:45.882089Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m M, N, K \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m\n\u001b[1;32m     40\u001b[0m block_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m]\n\u001b[0;32m---> 41\u001b[0m benchmark_results \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Print benchmark results\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfiguration\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<25\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvg Time (s)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThroughput (GB/s)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mbenchmark_matmul\u001b[0;34m(M, N, K, block_sizes, repetitions)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbenchmark_matmul\u001b[39m(M, N, K, block_sizes, repetitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Initialize matrices\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((K, N), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      9\u001b[0m     results \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/PycharmProjects/nvidia/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:305\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Benchmark function for Triton vs PyTorch CUDA\n",
    "def benchmark_matmul(M, N, K, block_sizes, repetitions=10):\n",
    "    # Initialize matrices\n",
    "    a = torch.rand((M, K), device='cuda', dtype=torch.float32)\n",
    "    b = torch.rand((K, N), device='cuda', dtype=torch.float32)\n",
    "    results = {}\n",
    "\n",
    "    # Triton matrix multiplication with varying block sizes\n",
    "    for block_size in block_sizes:\n",
    "        triton_times = []\n",
    "        for _ in range(repetitions):\n",
    "            start = time.time()\n",
    "            matmul_tiled(a, b, BLOCK_SIZE=block_size)\n",
    "            torch.cuda.synchronize()\n",
    "            triton_times.append(time.time() - start)\n",
    "        \n",
    "        avg_time = sum(triton_times) / repetitions\n",
    "        gbps = 2 * M * N * K * a.element_size() * 1e-9 / avg_time\n",
    "        results[f'Triton (BLOCK_SIZE={block_size})'] = (avg_time, gbps)\n",
    "\n",
    "    # PyTorch CUDA matrix multiplication\n",
    "    cuda_times = []\n",
    "    for _ in range(repetitions):\n",
    "        start = time.time()\n",
    "        torch.mm(a, b)\n",
    "        torch.cuda.synchronize()\n",
    "        cuda_times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = sum(cuda_times) / repetitions\n",
    "    gbps = 2 * M * N * K * a.element_size() * 1e-9 / avg_time\n",
    "    results['CUDA (Torch)'] = (avg_time, gbps)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define matrix dimensions and block sizes for testing\n",
    "M, N, K = 1024, 1024, 1024\n",
    "block_sizes = [64, 128, 256]\n",
    "benchmark_results = benchmark_matmul(M, N, K, block_sizes)\n",
    "\n",
    "# Print benchmark results\n",
    "print(f\"{'Configuration':<25} {'Avg Time (s)':<15} {'Throughput (GB/s)':<20}\")\n",
    "for config, (avg_time, gbps) in benchmark_results.items():\n",
    "    print(f\"{config:<25} {avg_time:<15.5f} {gbps:<20.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7766dcefe7c0455d",
   "metadata": {},
   "source": [
    "### Plotting the Results\n",
    "\n",
    "We will visualize the throughput (GB/s) for each block size to analyze the effect of tiling and blocking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe2572ff6f6d98b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:02:58.584229Z",
     "start_time": "2024-10-31T19:02:58.572740Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'benchmark_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot the throughput results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m configurations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mbenchmark_results\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      3\u001b[0m throughput_values \u001b[38;5;241m=\u001b[39m [benchmark_results[config][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m configurations]\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'benchmark_results' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the throughput results\n",
    "configurations = list(benchmark_results.keys())\n",
    "throughput_values = [benchmark_results[config][1] for config in configurations]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(configurations, throughput_values, color=['teal'] * len(block_sizes) + ['darkorange'], width=0.5)\n",
    "plt.xlabel(\"Configuration\", fontsize=14)\n",
    "plt.ylabel(\"Throughput (GB/s)\", fontsize=14)\n",
    "plt.title(\"Matrix Multiplication Throughput: Triton Block Sizes vs CUDA (Torch)\", fontsize=16)\n",
    "plt.xticks(fontsize=12, rotation=45)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Annotate throughput values on the bars\n",
    "for i, v in enumerate(throughput_values):\n",
    "    plt.text(i, v + 0.5, f\"{v:.2f} GB/s\", ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
