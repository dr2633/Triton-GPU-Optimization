{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction to GPU Architecture \n",
    "\n",
    "In this notebook, we'll get familiar with the basics of GPU architecture and using Triton for efficient GPU programming. \n",
    "\n",
    "The purpose of this notebook is to help users onboard to developing custom kernels in Python using Google Colab to access GPUs. \n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "- **Cores**: GPUs contain thousands of small cores designed for parallel processing.\n",
    "- **Memory Hierarchy**: \n",
    "  - **Global Memory**: Main memory, which is slower but accessible by all cores.\n",
    "  - **Shared Memory**: Fast, small, and accessible only by cores within the same thread block.\n",
    "  - **Registers**: The fastest memory, used for temporary storage within each thread.\n",
    "    \n",
    "\n",
    "- **Thread Blocks**:\n",
    "\n",
    "A thread block is a group of threads that execute concurrently and can communicate with each other through shared memory.\n",
    "\n",
    "In Triton, similar to CUDA, thread blocks allow developers to structure workloads efficiently by grouping threads that process a subset of data. Threads within the same block can synchronize and share data through shared memory, which is high-speed memory accessible to all threads in the block.\n",
    "\n",
    "**Why Thread Blocks?** By dividing the overall workload into blocks, GPUs can process data in parallel, where each thread performs part of the computation. This parallelism enables GPUs to handle large data sets much faster than serial CPU processing.\n",
    "\n",
    "**Execution in Triton**: In Triton kernels, users define the grid and block size to control work distribution across the GPU. The number of threads within each block and the number of blocks in a grid are crucial for optimizing performance, as they determine memory access patterns and processing efficiency.\n",
    "\n",
    "**Example**: In image processing, each thread might represent a single pixel. By processing all pixels in parallel, a GPU can efficiently handle high-resolution images in real-time.\n",
    "\n",
    "\n",
    "**Block size** \n",
    "\n",
    "Block size is the number of elements that a single Triton program instance operates on simultaneously.\n",
    "\n",
    "It's typically defined as a power of two (e.g., 128, 256, 512) for optimal performance."
   ],
   "id": "65b261ab612bde2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setting up Google Colab for Triton\n",
    "\n",
    "To run Triton code in Google Colab, follow these setup steps:\n",
    "\n",
    "1. **Enable GPU**:\n",
    "   - Go to **Runtime > Change runtime type**.\n",
    "   - Set **Hardware accelerator** to **GPU**, then click **Save**.\n",
    "\n",
    "2. **Install Triton**:\n",
    "   - Run the following command in the next cell to install Triton.\n"
   ],
   "id": "5be590f819f0fdf1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T15:49:03.872162Z",
     "start_time": "2024-10-29T15:49:03.322866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install Triton\n",
    "!pip install triton"
   ],
   "id": "4b5c3aab05f26596",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: Could not find a version that satisfies the requirement triton (from versions: none)\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for triton\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Verifying GPU Availability\n",
    "\n",
    "Let's check if a GPU is available in this Colab environment. We can use `torch.cuda.is_available()` to confirm. If a GPU is detected, we’ll print its name."
   ],
   "id": "9c4f891b4aaabb9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:17:59.788653Z",
     "start_time": "2024-10-29T16:17:57.200144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU found. Please enable GPU under Runtime > Change runtime type.\")"
   ],
   "id": "fdf09ceb3fde6fbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please enable GPU under Runtime > Change runtime type.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Writing a Simple Triton Kernel\n",
    "\n",
    "Triton makes it easy to write GPU kernels with a Pythonic interface. We'll start with a basic operation: **vector addition**.\n",
    "\n",
    "### Vector Addition\n",
    "\n",
    "Consider two vectors, \\(A\\) and \\(B\\), each with \\(N\\) elements. We want to compute their element-wise sum to produce a new vector, \\(C\\), where each element is defined by:\n",
    "\n",
    "$\n",
    "C[i] = A[i] + B[i]\n",
    "$\n",
    "\n",
    "This is a great starting point for understanding GPU parallelization, as each element addition is independent and can be done in parallel.\n"
   ],
   "id": "fdc21348b5dfc5d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T17:30:28.194198Z",
     "start_time": "2024-10-29T17:30:28.122844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import Triton libraries\n",
    "\n",
    "import triton \n",
    "import triton.language as tl \n",
    "\n",
    "@titon.jit\n",
    "def vector_add_kernel(\n",
    "    A_ptr, B_ptr, C_ptr, N, \n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    # Compute programmatically unique index for each thread \n",
    "    idx = tl.arange(0,BLOCK_SIZE) + tl.program_id(0) * BLOCK_SIZE\n",
    "\n",
    "    #The resulting idx is a tensor of indices, where each element corresponds to a unique global thread ID. \n",
    "    \n",
    "    #1. Determine which data elements each thread should process.\n",
    "    \n",
    "    #2. Ensure that threads across different blocks don't overlap in their computations.\n",
    "    \n",
    "    #3. Create a mapping between threads and data elements in a way that scales with the grid size.\n",
    "    \n",
    "    # Set mask to avoid out-of-bounds access\n",
    "    mask = idx < N\n",
    "    \n",
    "    # Load data from pointers A and B, add them, and store to C\n",
    "    a = tl.load(A_ptr + idx, mask=mask)\n",
    "    b = tl.load(B_ptr + idx, mask=mask)\n",
    "    c = a + b\n",
    "    tl.store(C_ptr + idx, c, mask=mask)\n",
    "    "
   ],
   "id": "e5a6304f670309ea",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Import Triton libraries\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtriton\u001B[39;00m \n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtriton\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlanguage\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtl\u001B[39;00m \n\u001B[1;32m      6\u001B[0m \u001B[38;5;129m@titon\u001B[39m\u001B[38;5;241m.\u001B[39mjit\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvector_add_kernel\u001B[39m(\n\u001B[1;32m      8\u001B[0m     A_ptr, B_ptr, C_ptr, N, \n\u001B[1;32m      9\u001B[0m     BLOCK_SIZE: tl\u001B[38;5;241m.\u001B[39mconstexpr\n\u001B[1;32m     10\u001B[0m ):\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;66;03m# Compute programmatically unique index for each thread \u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'triton'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:54:29.414021Z",
     "start_time": "2024-10-29T16:54:29.410221Z"
    }
   },
   "cell_type": "code",
   "source": "# Best practices for validating and benchmarking your custom ops against native reference implementations.",
   "id": "1eaad5de257b6722",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Understanding the Kernel \n",
    "\n",
    "In this kernel: \n",
    "\n",
    "- **`tl.arange(0, BLOCK_SIZE)`**: Creates a range of indices for each thread block \n",
    "- **`tl.program_id(0)`**: Returns a unique identifier for each program instance, allowing us to compute a global index for each thread.\n",
    "- - **`tl.load` and `tl.store`**: Efficiently load from and store to global memory, respectively, using the pointers `A_ptr`, `B_ptr`, and `C_ptr`.\n",
    "- **`mask`**: Ensures that we don’t access out-of-bounds memory when the array length isn’t a perfect multiple of the block size.\n",
    "\n",
    "\n",
    "Each thread computes one element of the vector sum in parallel. Now let’s initialize some data and launch the kernel!\n",
    "\n"
   ],
   "id": "c161188d26947efd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Executing the Kernel \n",
    "\n",
    "Now, let’s initialize some vectors and execute the kernel to see Triton in action. We’ll output the resulting vector as a confirmation that our kernel is working.\n"
   ],
   "id": "fa64d97ebd0a4a4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T17:32:42.918371Z",
     "start_time": "2024-10-29T17:32:42.882641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Initialize input vectors A and B with N elements\n",
    "N = 1024\n",
    "A = torch.rand(N, device='cuda')\n",
    "B = torch.rand(N, device='cuda')\n",
    "C = torch.empty(N, device='cuda')\n",
    "\n",
    "\n",
    "# Launch the kernel\n",
    "BLOCK_SIZE = 128\n",
    "grid = (N + BLOCK_SIZE - 1) // BLOCK_SIZE  # Number of program instances\n",
    "vector_add_kernel[grid](A, B, C, N, BLOCK_SIZE=BLOCK_SIZE)\n",
    "\n",
    "# Print \"Hello, GPU!\" message and a slice of the output vector\n",
    "print(\"Hello, GPU!\")\n",
    "print(\"Result of A + B (first 10 elements):\", C[:10].cpu().numpy())"
   ],
   "id": "fa6ceca4021e6d53",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Initialize input vectors A and B with N elements\u001B[39;00m\n\u001B[1;32m      4\u001B[0m N \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1024\u001B[39m\n\u001B[0;32m----> 5\u001B[0m A \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrand\u001B[49m\u001B[43m(\u001B[49m\u001B[43mN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m B \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand(N, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m C \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mempty(N, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/nvidia/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:305\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    301\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    302\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    303\u001B[0m     )\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    306\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    308\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    309\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6b3128b1c1baaa8c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
