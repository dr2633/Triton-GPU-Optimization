{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "65b261ab612bde2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setting up Google Colab for Triton\n",
    "\n",
    "To run Triton code in Google Colab, follow these setup steps:\n",
    "\n",
    "1. **Enable GPU**:\n",
    "   - Go to **Runtime > Change runtime type**.\n",
    "   - Set **Hardware accelerator** to **GPU**, then click **Save**.\n",
    "\n",
    "2. **Install Triton**:\n",
    "   - Run the following command in the next cell to install Triton.\n"
   ],
   "id": "5be590f819f0fdf1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction to GPU Architecture \n",
    "\n",
    "In this notebook, we'll explore the basics of GPU architecture and introduce **Triton**, a Python library that makes it easier to write efficient GPU programs. \n",
    "\n",
    "The purpose of this notebook is to help users onboard to developing custom GPU kernels in Python, using **Google Colab** to access GPU resources. By the end, you’ll have a foundational understanding of how GPUs work and be ready to write your first Triton kernel.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "- **Cores**: \n",
    "  - GPUs are designed with thousands of small processing units called **cores**. Each core can execute operations simultaneously, making GPUs ideal for parallel processing tasks like deep learning and scientific computing.\n",
    "  - This parallelism allows a GPU to handle many operations at once, providing massive speedup over serial processing on CPUs.\n",
    "\n",
    "- **Memory Hierarchy**: \n",
    "  - **Global Memory**: \n",
    "    - This is the main memory accessible by all cores. It has a large capacity but is relatively slow. Global memory is often used to store large datasets, like images or matrices, that threads will work on.\n",
    "  - **Shared Memory**:\n",
    "    - A small, high-speed memory accessible only by cores within the same thread block. Shared memory is critical for operations where multiple threads need to access or modify the same data, such as matrix multiplication.\n",
    "  - **Registers**:\n",
    "    - Registers are the fastest type of memory, used for temporary data storage within each thread. They are private to each thread and offer minimal latency, making them ideal for frequently accessed data.\n",
    "\n",
    "- **Thread Blocks**:\n",
    "  - A **thread block** is a group of threads that execute concurrently and can communicate with each other through **shared memory**.\n",
    "  - In Triton (similar to CUDA), thread blocks allow developers to structure workloads efficiently by grouping threads to process subsets of data. Threads within the same block can synchronize and share data through shared memory, a high-speed memory accessible to all threads in the block.\n",
    "\n",
    "  - **Why Thread Blocks?**\n",
    "    - By dividing the overall workload into thread blocks, GPUs can process data in parallel, where each thread performs a part of the computation. This parallelism enables GPUs to handle large datasets much faster than serial CPU processing.\n",
    "\n",
    "  - **Execution in Triton**:\n",
    "    - In Triton kernels, users define the **grid** and **block size** to control work distribution across the GPU. The number of threads within each block and the number of blocks in a grid are crucial for optimizing performance, as they determine memory access patterns and processing efficiency.\n",
    "\n",
    "  - **Example**:\n",
    "    - In image processing, each thread might represent a single pixel. By processing all pixels in parallel, a GPU can efficiently handle high-resolution images in real-time, allowing for quick operations on each pixel independently.\n",
    "\n",
    "- **Block Size**:\n",
    "  - **Block size** refers to the number of elements that a single Triton program instance operates on simultaneously. \n",
    "  - It’s typically defined as a power of two (e.g., 128, 256, 512) for optimal performance, as this aligns well with the GPU’s memory structure and access patterns. The choice of block size can have a significant impact on performance and memory efficiency, so it’s often adjusted based on the task and data size.\n",
    "\n",
    "---\n",
    "\n",
    "With these foundational concepts in mind, we’ll start by setting up Triton in Colab and verifying that our GPU is ready to use. Let’s dive in!\n"
   ],
   "id": "5aca05b6080533ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setting up Google Colab for Triton\n",
    "\n",
    "To run Triton code in Google Colab, follow these setup steps:\n",
    "\n",
    "1. **Enable GPU**:\n",
    "   - Go to **Runtime > Change runtime type**.\n",
    "   - Set **Hardware accelerator** to **GPU**, then click **Save**.\n",
    "\n",
    "2. **Install Triton**:\n",
    "   - Run the following command in the next cell to install Triton."
   ],
   "id": "755419b543c0353a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:44:05.923722Z",
     "start_time": "2024-10-29T21:44:05.920161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install Triton\n",
    "!pip install triton"
   ],
   "id": "4b5c3aab05f26596",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Verifying GPU Availability\n",
    "\n",
    "Let's check if a GPU is available in this Colab environment. We can use `torch.cuda.is_available()` to confirm. If a GPU is detected, we’ll print its name."
   ],
   "id": "9c4f891b4aaabb9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:41:28.751040Z",
     "start_time": "2024-10-29T21:41:28.747792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU found. Please enable GPU under Runtime > Change runtime type.\")"
   ],
   "id": "fdf09ceb3fde6fbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please enable GPU under Runtime > Change runtime type.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Writing a Simple Triton Kernel\n",
    "\n",
    "Triton makes it easy to write GPU kernels with a Pythonic interface. We'll start with a basic operation: **vector addition**.\n",
    "\n",
    "### Vector Addition\n",
    "\n",
    "Consider two vectors, $A$ and $B$, each with $N$ elements. We want to compute their element-wise sum to produce a new vector, $C$, where each element is defined by:\n",
    "\n",
    "$C[i] = A[i] + B[i]$\n",
    "\n",
    "This is a great starting point for understanding GPU parallelization, as each element addition is independent and can be done in parallel.\n"
   ],
   "id": "fdc21348b5dfc5d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T17:30:28.194198Z",
     "start_time": "2024-10-29T17:30:28.122844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import Triton libraries for writing and running GPU kernels\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# Step 1: Define the kernel function for vector addition.\n",
    "# This kernel will add elements from two input vectors, A and B,\n",
    "# and store the result in a third vector, C.\n",
    "@triton.jit\n",
    "def vector_add_kernel(A_ptr, B_ptr, C_ptr, N, BLOCK_SIZE: tl.constexpr):\n",
    "    \n",
    "    # Generate unique indices for each thread within the block.\n",
    "    # `tl.arange(0, BLOCK_SIZE)` produces a range of local indices within each block,\n",
    "    # and `tl.program_id(0)` is the block ID, ensuring a unique index per thread globally.\n",
    "    idx = tl.arange(0, BLOCK_SIZE) + tl.program_id(0) * BLOCK_SIZE\n",
    "\n",
    "    # Set a mask to prevent threads from accessing out-of-bounds memory.\n",
    "    # Only threads with indices < N will load and process data.\n",
    "    mask = idx < N\n",
    "    \n",
    "    # Load data from global memory at the computed indices.\n",
    "    # `tl.load` fetches elements from A and B, using the mask to avoid invalid accesses.\n",
    "    a = tl.load(A_ptr + idx, mask=mask)\n",
    "    b = tl.load(B_ptr + idx, mask=mask)\n",
    "    \n",
    "    # Perform element-wise addition of vectors A and B.\n",
    "    # Each thread calculates one element of the result in parallel.\n",
    "    c = a + b\n",
    "    \n",
    "    # Store the result in vector C at the corresponding index, with masking.\n",
    "    # `tl.store` writes each result back to global memory.\n",
    "    tl.store(C_ptr + idx, c, mask=mask)"
   ],
   "id": "e5a6304f670309ea",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Import Triton libraries\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtriton\u001B[39;00m \n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtriton\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlanguage\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtl\u001B[39;00m \n\u001B[1;32m      6\u001B[0m \u001B[38;5;129m@titon\u001B[39m\u001B[38;5;241m.\u001B[39mjit\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvector_add_kernel\u001B[39m(\n\u001B[1;32m      8\u001B[0m     A_ptr, B_ptr, C_ptr, N, \n\u001B[1;32m      9\u001B[0m     BLOCK_SIZE: tl\u001B[38;5;241m.\u001B[39mconstexpr\n\u001B[1;32m     10\u001B[0m ):\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;66;03m# Compute programmatically unique index for each thread \u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'triton'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Understanding the Kernel\n",
    "\n",
    "In this kernel, we use several Triton functions to perform vector addition in parallel across the GPU. Here’s a breakdown of the core functions:\n",
    "\n",
    "- **`tl.arange(0, BLOCK_SIZE)`**: Generates a range of indices for each thread within the block, from `0` to `BLOCK_SIZE - 1`. This allows each thread to identify which part of the data it will work on.\n",
    "\n",
    "- **`tl.program_id(0)`**: Returns a unique identifier for each program instance (or thread block). By multiplying this identifier by `BLOCK_SIZE`, we compute a unique starting index for each thread block, ensuring that threads operate on different data segments.\n",
    "\n",
    "- **`tl.load` and `tl.store`**: These functions handle memory access:\n",
    "    - **`tl.load`**: Reads data from global memory into the kernel for processing. In this case, it loads elements from `A_ptr` and `B_ptr`.\n",
    "    - **`tl.store`**: Writes processed data back to global memory. Here, it stores the result of the addition in `C_ptr`.\n",
    "\n",
    "- **`mask`**: Ensures that we don’t access out-of-bounds memory when the array length isn’t a perfect multiple of the block size. \n",
    "\n",
    "> **Note**: In addition to safety, masking also helps with **memory efficiency** by preventing unnecessary data access. This reduces memory bandwidth usage, as only the valid indices are accessed.\n",
    "\n",
    "Each thread computes one element of the vector sum independently, allowing the GPU to process large vectors in parallel efficiently. \n",
    "\n",
    "Now, let’s initialize some data and launch the kernel!\n"
   ],
   "id": "c161188d26947efd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Executing the Kernel \n",
    "\n",
    "Now, let’s initialize some vectors and execute the kernel to see Triton in action. We’ll create two random input vectors, `A` and `B`, and an empty output vector, `C`. Then we’ll launch our `vector_add_kernel` to compute the element-wise sum of `A` and `B` in parallel on the GPU. Finally, we’ll print out a \"Hello, GPU!\" message and display a portion of the result to confirm that the kernel worked as expected.\n"
   ],
   "id": "fa64d97ebd0a4a4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:59:06.440430Z",
     "start_time": "2024-10-29T21:59:06.411773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Initialize input vectors A and B with N elements\n",
    "# These vectors are created on the GPU using torch's `device='cuda'`\n",
    "N = 1024\n",
    "A = torch.rand(N, device='cuda')    # Random values in vector A\n",
    "B = torch.rand(N, device='cuda')    # Random values in vector B\n",
    "C = torch.empty(N, device='cuda')   # Empty vector C for storing the result\n",
    "\n",
    "# Launch the kernel\n",
    "# BLOCK_SIZE defines how many elements each thread block processes\n",
    "BLOCK_SIZE = 128\n",
    "grid = (N + BLOCK_SIZE - 1) // BLOCK_SIZE  # Calculate the number of blocks needed\n",
    "\n",
    "# Execute the kernel with the specified grid size and block size\n",
    "vector_add_kernel[grid](A, B, C, N, BLOCK_SIZE=BLOCK_SIZE)\n",
    "\n",
    "# Print \"Hello, GPU!\" message and show the first 10 elements of the result\n",
    "print(\"Hello, GPU!\")\n",
    "print(\"Result of A + B (first 10 elements):\", C[:10].cpu().numpy())"
   ],
   "id": "fa6ceca4021e6d53",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Initialize input vectors A and B with N elements\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# These vectors are created on the GPU using torch's `device='cuda'`\u001B[39;00m\n\u001B[1;32m      5\u001B[0m N \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1024\u001B[39m\n\u001B[0;32m----> 6\u001B[0m A \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrand\u001B[49m\u001B[43m(\u001B[49m\u001B[43mN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m    \u001B[38;5;66;03m# Random values in vector A\u001B[39;00m\n\u001B[1;32m      7\u001B[0m B \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand(N, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)    \u001B[38;5;66;03m# Random values in vector B\u001B[39;00m\n\u001B[1;32m      8\u001B[0m C \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mempty(N, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)   \u001B[38;5;66;03m# Empty vector C for storing the result\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/nvidia/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:305\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    301\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    302\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    303\u001B[0m     )\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    306\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    308\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    309\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Comparing Performance: Triton vs. PyTorch (CUDA)\n",
    "\n",
    "To understand Triton’s potential advantages, we’ll compare the performance of a vector addition operation using Triton and PyTorch (CUDA). By running each approach multiple times, we’ll observe the time differences and highlight Triton’s efficiency for this basic operation.\n"
   ],
   "id": "79f7916abc1629ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T22:02:41.742002Z",
     "start_time": "2024-10-29T22:02:41.723109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Triton Vector Addition Kernel (Same as the one we defined previously)\n",
    "@triton.jit\n",
    "def vector_add_kernel(A_ptr, B_ptr, C_ptr, N, BLOCK_SIZE: tl.constexpr):\n",
    "    idx = tl.arange(0, BLOCK_SIZE) + tl.program_id(0) * BLOCK_SIZE\n",
    "    mask = idx < N\n",
    "    a = tl.load(A_ptr + idx, mask=mask)\n",
    "    b = tl.load(B_ptr + idx, mask=mask)\n",
    "    c = a + b\n",
    "    tl.store(C_ptr + idx, c, mask=mask)\n",
    "\n",
    "\n",
    "# Function to execute Triton kernel and measure time\n",
    "def triton_vector_add(A, B, C, N, BLOCK_SIZE=128):\n",
    "    grid = (N + BLOCK_SIZE - 1) // BLOCK_SIZE\n",
    "    # Start timing\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    vector_add_kernel[grid](A, B, C, N, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return start.elapsed_time(end)  # Time in milliseconds\n"
   ],
   "id": "ca0ec97c2743759b",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'triton' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Triton Vector Addition Kernel (Same as the one we defined previously)\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;129m@triton\u001B[39m\u001B[38;5;241m.\u001B[39mjit\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvector_add_kernel\u001B[39m(A_ptr, B_ptr, C_ptr, N, BLOCK_SIZE: tl\u001B[38;5;241m.\u001B[39mconstexpr):\n\u001B[1;32m      4\u001B[0m     idx \u001B[38;5;241m=\u001B[39m tl\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;241m0\u001B[39m, BLOCK_SIZE) \u001B[38;5;241m+\u001B[39m tl\u001B[38;5;241m.\u001B[39mprogram_id(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m*\u001B[39m BLOCK_SIZE\n\u001B[1;32m      5\u001B[0m     mask \u001B[38;5;241m=\u001B[39m idx \u001B[38;5;241m<\u001B[39m N\n",
      "\u001B[0;31mNameError\u001B[0m: name 'triton' is not defined"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# PyTorch CUDA Vector Addition\n",
    "def cuda_vector_add(A, B):\n",
    "    # Start timing\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    C = A + B\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return start.elapsed_time(end)  # Time in milliseconds"
   ],
   "id": "9285b469f470a1c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize data and run comparison \n",
    "import torch\n",
    "\n",
    "# Initialize vectors on GPU\n",
    "N = 1024 * 1024  # Use a large N to better observe performance differences\n",
    "A = torch.rand(N, device='cuda')\n",
    "B = torch.rand(N, device='cuda')\n",
    "C = torch.empty(N, device='cuda')\n",
    "\n",
    "# Measure performance over multiple runs and average results\n",
    "triton_times = [triton_vector_add(A, B, C, N) for _ in range(10)]\n",
    "cuda_times = [cuda_vector_add(A, B) for _ in range(10)]\n",
    "\n",
    "avg_triton_time = sum(triton_times) / len(triton_times)\n",
    "avg_cuda_time = sum(cuda_times) / len(cuda_times)\n",
    "\n",
    "print(f\"Average Triton time: {avg_triton_time:.3f} ms\")\n",
    "print(f\"Average CUDA time: {avg_cuda_time:.3f} ms\")\n"
   ],
   "id": "2f41eee266834e22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the results\n",
    "plt.bar([\"Triton\", \"CUDA (PyTorch)\"], [avg_triton_time, avg_cuda_time], color=['royalblue', 'darkorange'])\n",
    "plt.ylabel(\"Average Time (ms)\")\n",
    "plt.title(\"Performance Comparison: Triton vs. CUDA (PyTorch)\")\n",
    "plt.show()"
   ],
   "id": "c26f8a5f2206ea3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
