{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "65b261ab612bde2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction to GPU Architecture\n",
    "\n",
    "In this notebook, we'll explore the basics of GPU architecture and introduce **Triton**, a Python library that makes it easier to write efficient GPU programs.\n",
    "\n",
    "The purpose of this notebook is to help users onboard to developing custom GPU kernels in Python, using **Google Colab** to access GPU resources. By the end, you’ll have a foundational understanding of how GPUs work and be ready to write your first Triton kernel.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "- **Cores**:\n",
    "  - GPUs are designed with thousands of small processing units called **cores**. Each core can execute operations simultaneously, making GPUs ideal for parallel processing tasks like deep learning and scientific computing.\n",
    "  - This parallelism allows a GPU to handle many operations at once, providing massive speedup over serial processing on CPUs.\n",
    "\n",
    "- **Memory Hierarchy**:\n",
    "  - **Global Memory**:\n",
    "    - This is the main memory accessible by all cores. It has a large capacity but is relatively slow. Global memory is often used to store large datasets, like images or matrices, that threads will work on.\n",
    "  - **Shared Memory**:\n",
    "    - A small, high-speed memory accessible only by cores within the same thread block. Shared memory is critical for operations where multiple threads need to access or modify the same data, such as matrix multiplication.\n",
    "  - **Registers**:\n",
    "    - Registers are the fastest type of memory, used for temporary data storage within each thread. They are private to each thread and offer minimal latency, making them ideal for frequently accessed data.\n",
    "\n",
    "- **Thread Blocks**:\n",
    "  - A **thread block** is a group of threads that execute concurrently and can communicate with each other through **shared memory**.\n",
    "  - In Triton (similar to CUDA), thread blocks allow developers to structure workloads efficiently by grouping threads to process subsets of data. Threads within the same block can synchronize and share data through shared memory, a high-speed memory accessible to all threads in the block.\n",
    "\n",
    "  - **Why Thread Blocks?**\n",
    "    - By dividing the overall workload into thread blocks, GPUs can process data in parallel, where each thread performs a part of the computation. This parallelism enables GPUs to handle large datasets much faster than serial CPU processing.\n",
    "\n",
    "  - **Execution in Triton**:\n",
    "    - In Triton kernels, users define the **grid** and **block size** to control work distribution across the GPU. The number of threads within each block and the number of blocks in a grid are crucial for optimizing performance, as they determine memory access patterns and processing efficiency.\n",
    "\n",
    "  - **Example**:\n",
    "    - In image processing, each thread might represent a single pixel. By processing all pixels in parallel, a GPU can efficiently handle high-resolution images in real-time, allowing for quick operations on each pixel independently.\n",
    "\n",
    "- **Block Size**:\n",
    "  - **Block size** refers to the number of elements that a single Triton program instance operates on simultaneously.\n",
    "  - It’s typically defined as a power of two (e.g., 128, 256, 512) for optimal performance, as this aligns well with the GPU’s memory structure and access patterns. The choice of block size can have a significant impact on performance and memory efficiency, so it’s often adjusted based on the task and data size.\n",
    "\n",
    "---\n",
    "\n",
    "With these foundational concepts in mind, we’ll start by setting up Triton in Colab and verifying that our GPU is ready to use. Let’s dive in!"
   ],
   "id": "5aca05b6080533ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setting up Google Colab for Triton\n",
    "\n",
    "To run Triton code in Google Colab, follow these setup steps:\n",
    "\n",
    "1. **Enable GPU**:\n",
    "   - Go to **Runtime > Change runtime type**.\n",
    "   - Set **Hardware accelerator** to **GPU**, then click **Save**.\n",
    "\n",
    "2. **Install Triton**:\n",
    "   - Run the following command in the next cell to install Triton."
   ],
   "id": "755419b543c0353a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:44:05.923722Z",
     "start_time": "2024-10-29T21:44:05.920161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install Triton\n",
    "!pip install triton"
   ],
   "id": "4b5c3aab05f26596",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Verifying GPU Availability\n",
    "\n",
    "Let's check if a GPU is available in this Colab environment. We can use `torch.cuda.is_available()` to confirm. If a GPU is detected, we’ll print its name."
   ],
   "id": "9c4f891b4aaabb9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:41:28.751040Z",
     "start_time": "2024-10-29T21:41:28.747792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU found. Please enable GPU under Runtime > Change runtime type.\")"
   ],
   "id": "fdf09ceb3fde6fbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please enable GPU under Runtime > Change runtime type.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Writing a Simple Triton Kernel\n",
    "\n",
    "Triton makes it easy to write GPU kernels with a Pythonic interface. We'll start with a basic operation: **vector addition**.\n",
    "\n",
    "### Vector Addition\n",
    "\n",
    "Consider two vectors, $A$ and $B$, each with $N$ elements. We want to compute their element-wise sum to produce a new vector, $C$, where each element is defined by:\n",
    "\n",
    "$C[i] = A[i] + B[i]$\n",
    "\n",
    "This is a great starting point for understanding GPU parallelization, as each element addition is independent and can be done in parallel.\n"
   ],
   "id": "fdc21348b5dfc5d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T17:30:28.194198Z",
     "start_time": "2024-10-29T17:30:28.122844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import Triton libraries for writing and running GPU kernels\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# Step 1: Define the kernel function for vector addition.\n",
    "# This kernel will add elements from two input vectors, A and B,\n",
    "# and store the result in a third vector, C.\n",
    "@triton.jit\n",
    "def vector_add_kernel(A_ptr, B_ptr, C_ptr, N, BLOCK_SIZE: tl.constexpr):\n",
    "\n",
    "    # Generate unique indices for each thread within the block.\n",
    "    # `tl.arange(0, BLOCK_SIZE)` produces a range of local indices within each block,\n",
    "    # and `tl.program_id(0)` is the block ID, ensuring a unique index per thread globally.\n",
    "    idx = tl.arange(0, BLOCK_SIZE) + tl.program_id(0) * BLOCK_SIZE\n",
    "\n",
    "    # Set a mask to prevent threads from accessing out-of-bounds memory.\n",
    "    # Only threads with indices < N will load and process data.\n",
    "    mask = idx < N\n",
    "\n",
    "    # Load data from global memory at the computed indices.\n",
    "    # `tl.load` fetches elements from A and B, using the mask to avoid invalid accesses.\n",
    "    a = tl.load(A_ptr + idx, mask=mask)\n",
    "    b = tl.load(B_ptr + idx, mask=mask)\n",
    "\n",
    "    # Perform element-wise addition of vectors A and B.\n",
    "    # Each thread calculates one element of the result in parallel.\n",
    "    c = a + b\n",
    "\n",
    "    # Store the result in vector C at the corresponding index, with masking.\n",
    "    # `tl.store` writes each result back to global memory.\n",
    "    tl.store(C_ptr + idx, c, mask=mask)"
   ],
   "id": "e5a6304f670309ea",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Import Triton libraries\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtriton\u001B[39;00m \n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtriton\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlanguage\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtl\u001B[39;00m \n\u001B[1;32m      6\u001B[0m \u001B[38;5;129m@titon\u001B[39m\u001B[38;5;241m.\u001B[39mjit\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvector_add_kernel\u001B[39m(\n\u001B[1;32m      8\u001B[0m     A_ptr, B_ptr, C_ptr, N, \n\u001B[1;32m      9\u001B[0m     BLOCK_SIZE: tl\u001B[38;5;241m.\u001B[39mconstexpr\n\u001B[1;32m     10\u001B[0m ):\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;66;03m# Compute programmatically unique index for each thread \u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'triton'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Understanding the Kernel\n",
    "\n",
    "In this kernel, we use several Triton functions to perform vector addition in parallel across the GPU. Here’s a breakdown of the core functions:\n",
    "\n",
    "- **`tl.arange(0, BLOCK_SIZE)`**: Generates a range of indices for each thread within the block, from `0` to `BLOCK_SIZE - 1`. This allows each thread to identify which part of the data it will work on.\n",
    "\n",
    "- **`tl.program_id(0)`**: Returns a unique identifier for each program instance (or thread block). By multiplying this identifier by `BLOCK_SIZE`, we compute a unique starting index for each thread block, ensuring that threads operate on different data segments.\n",
    "\n",
    "- **`tl.load` and `tl.store`**: These functions handle memory access:\n",
    "    - **`tl.load`**: Reads data from global memory into the kernel for processing. In this case, it loads elements from `A_ptr` and `B_ptr`.\n",
    "    - **`tl.store`**: Writes processed data back to global memory. Here, it stores the result of the addition in `C_ptr`.\n",
    "\n",
    "- **`mask`**: Ensures that we don’t access out-of-bounds memory when the array length isn’t a perfect multiple of the block size.\n",
    "\n",
    "> **Note**: In addition to safety, masking also helps with **memory efficiency** by preventing unnecessary data access. This reduces memory bandwidth usage, as only the valid indices are accessed.\n",
    "\n",
    "Each thread computes one element of the vector sum independently, allowing the GPU to process large vectors in parallel efficiently.\n",
    "\n",
    "Now, let’s initialize some data and launch the kernel!\n"
   ],
   "id": "c161188d26947efd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Executing the Kernel \n",
    "\n",
    "Now, let’s initialize some vectors and execute the kernel to see Triton in action. We’ll create two random input vectors, `A` and `B`, and an empty output vector, `C`. Then we’ll launch our `vector_add_kernel` to compute the element-wise sum of `A` and `B` in parallel on the GPU. Finally, we’ll print out a \"Hello, GPU!\" message and display a portion of the result to confirm that the kernel worked as expected.\n"
   ],
   "id": "fa64d97ebd0a4a4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:59:06.440430Z",
     "start_time": "2024-10-29T21:59:06.411773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Initialize input vectors A and B with N elements\n",
    "# These vectors are created on the GPU using torch's `device='cuda'`\n",
    "N = 1024\n",
    "A = torch.rand(N, device='cuda')    # Random values in vector A\n",
    "B = torch.rand(N, device='cuda')    # Random values in vector B\n",
    "C = torch.empty(N, device='cuda')   # Empty vector C for storing the result\n",
    "\n",
    "# Launch the kernel\n",
    "# BLOCK_SIZE defines how many elements each thread block processes\n",
    "BLOCK_SIZE = 128\n",
    "\n",
    "# Calculate the number of blocks needed as a tuple\n",
    "grid = ((N + BLOCK_SIZE - 1) // BLOCK_SIZE,)\n",
    "\n",
    "# Execute the kernel with the specified grid size and block size\n",
    "vector_add_kernel[grid](A, B, C, N, BLOCK_SIZE=BLOCK_SIZE)\n",
    "\n",
    "# Execute the kernel with the specified grid size and block size\n",
    "vector_add_kernel[grid](A, B, C, N, BLOCK_SIZE=BLOCK_SIZE)\n",
    "\n",
    "# Print \"Hello, GPU!\" message and show the first 10 elements of the result\n",
    "print(\"Hello, GPU!\")\n",
    "print(\"Result of A + B (first 10 elements):\", C[:10].cpu().numpy())"
   ],
   "id": "fa6ceca4021e6d53",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Initialize input vectors A and B with N elements\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# These vectors are created on the GPU using torch's `device='cuda'`\u001B[39;00m\n\u001B[1;32m      5\u001B[0m N \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1024\u001B[39m\n\u001B[0;32m----> 6\u001B[0m A \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrand\u001B[49m\u001B[43m(\u001B[49m\u001B[43mN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m    \u001B[38;5;66;03m# Random values in vector A\u001B[39;00m\n\u001B[1;32m      7\u001B[0m B \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand(N, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)    \u001B[38;5;66;03m# Random values in vector B\u001B[39;00m\n\u001B[1;32m      8\u001B[0m C \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mempty(N, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)   \u001B[38;5;66;03m# Empty vector C for storing the result\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/nvidia/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:305\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    301\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    302\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    303\u001B[0m     )\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    306\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    308\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    309\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Comparing Performance: Triton vs. PyTorch (CUDA)\n",
    "\n",
    "To understand Triton’s potential advantages, we’ll compare the performance of a vector addition operation using Triton and PyTorch (CUDA). By running each approach multiple times, we’ll observe the time differences and highlight Triton’s efficiency for this basic operation.\n"
   ],
   "id": "79f7916abc1629ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T22:02:41.742002Z",
     "start_time": "2024-10-29T22:02:41.723109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x + y\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "def add(x: torch.Tensor, y: torch.Tensor, BLOCK_SIZE=1024):\n",
    "    output = torch.empty_like(x)\n",
    "    n_elements = output.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    return output\n",
    "\n",
    "# Benchmark function with Triton and PyTorch (CUDA)\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['size'],\n",
    "        x_vals=[2**i for i in range(12, 28, 1)],\n",
    "        x_log=True,\n",
    "        line_arg='provider',\n",
    "        line_vals=['triton', 'torch'],\n",
    "        line_names=['Triton', 'Torch'],\n",
    "        styles=[('teal', '-'), ('darkorange', '-')],\n",
    "        ylabel='GB/s',\n",
    "        plot_name='vector-add-performance',\n",
    "        args={},\n",
    "    ))\n",
    "def benchmark(size, provider):\n",
    "    x = torch.rand(size, device='cuda', dtype=torch.float32)\n",
    "    y = torch.rand(size, device='cuda', dtype=torch.float32)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y, BLOCK_SIZE=512), quantiles=quantiles)\n",
    "\n",
    "    gbps = lambda ms: 3 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True)\n"
   ],
   "id": "ca0ec97c2743759b",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'triton' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Triton Vector Addition Kernel (Same as the one we defined previously)\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;129m@triton\u001B[39m\u001B[38;5;241m.\u001B[39mjit\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvector_add_kernel\u001B[39m(A_ptr, B_ptr, C_ptr, N, BLOCK_SIZE: tl\u001B[38;5;241m.\u001B[39mconstexpr):\n\u001B[1;32m      4\u001B[0m     idx \u001B[38;5;241m=\u001B[39m tl\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;241m0\u001B[39m, BLOCK_SIZE) \u001B[38;5;241m+\u001B[39m tl\u001B[38;5;241m.\u001B[39mprogram_id(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m*\u001B[39m BLOCK_SIZE\n\u001B[1;32m      5\u001B[0m     mask \u001B[38;5;241m=\u001B[39m idx \u001B[38;5;241m<\u001B[39m N\n",
      "\u001B[0;31mNameError\u001B[0m: name 'triton' is not defined"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fused Addition and Multiplication Kernel: Triton vs. PyTorch (CUDA)\n",
    "\n",
    "In this benchmark, we will evaluate a fused addition and multiplication kernel, where we perform an element-wise addition followed by multiplication with a scalar. The purpose of this fused operation is to reduce memory access overhead by combining two operations into a single kernel, which should enhance performance, especially for memory-bound tasks.\n",
    "\n",
    "### Steps in the Fused Kernel\n",
    "1. **Addition and Multiplication**: Each element in vectors `x` and `y` is added, and the result is multiplied by a scalar.\n",
    "2. **Memory Efficiency**: By fusing these operations, we minimize the number of times data is read from and written to memory, which is critical for performance.\n",
    "3. **Benchmark Comparison**: We compare the fused operation's performance between Triton and PyTorch (CUDA) to evaluate Triton's ability to optimize such fused operations.\n",
    "\n",
    "Below is the implementation and benchmark.\n"
   ],
   "id": "4d88f6e60bbbc75c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T23:44:30.192689Z",
     "start_time": "2024-10-29T23:44:30.161462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# Set a fixed random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Triton kernel for fused addition and multiplication\n",
    "@triton.jit\n",
    "def fused_add_mul_kernel(x_ptr, y_ptr, output_ptr, scalar, n_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    # Identify program ID for each block in the 1D grid\n",
    "    pid = tl.program_id(axis=0)\n",
    "    \n",
    "    # Calculate starting position for each thread block\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    # Mask to ensure we do not access out-of-bounds memory\n",
    "    mask = offsets < n_elements\n",
    "    \n",
    "    # Load elements from x and y using the mask\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    \n",
    "    # Perform the fused addition and multiplication operation\n",
    "    output = (x + y) * scalar\n",
    "    \n",
    "    # Store the result in output, using the mask for bounds safety\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "# Function to launch the fused kernel with Triton\n",
    "def fused_add_mul(x: torch.Tensor, y: torch.Tensor, scalar=2.0, BLOCK_SIZE=1024):\n",
    "    # Allocate output tensor\n",
    "    output = torch.empty_like(x)\n",
    "    n_elements = output.numel()\n",
    "    \n",
    "    # Define grid as a 1D tuple for the number of blocks\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "    \n",
    "    # Launch the Triton kernel\n",
    "    fused_add_mul_kernel[grid](x, y, output, scalar, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    return output\n",
    "\n",
    "# Benchmark function for comparing Triton vs. PyTorch for fused operations\n",
    "def benchmark_fused(size, provider, scalar=2.0, BLOCK_SIZE=1024):\n",
    "    # Initialize input tensors on the GPU\n",
    "    x = torch.rand(size, device='cuda', dtype=torch.float32)\n",
    "    y = torch.rand(size, device='cuda', dtype=torch.float32)\n",
    "    quantiles = [0.5, 0.2, 0.8]  # Define quantiles for performance measurement\n",
    "\n",
    "    # Execute the benchmark based on the provider (Triton or CUDA in PyTorch)\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: (x + y) * scalar, quantiles=quantiles)\n",
    "    elif provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: fused_add_mul(x, y, scalar, BLOCK_SIZE=BLOCK_SIZE), quantiles=quantiles)\n",
    "\n",
    "    # Calculate bandwidth in GB/s\n",
    "    gbps = lambda ms: 3 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "# Run the benchmark with Triton at different BLOCK_SIZE values\n",
    "for block_size in [128, 256, 512, 1024]:\n",
    "    print(f\"\\nBenchmarking fused add-multiply operation with BLOCK_SIZE = {block_size}\")\n",
    "    triton_result = benchmark_fused(size=1024*1024, provider='triton', scalar=2.0, BLOCK_SIZE=block_size)\n",
    "    print(f\"Triton (BLOCK_SIZE={block_size}) - Average: {triton_result[0]:.3f} GB/s, Max: {triton_result[1]:.3f} GB/s, Min: {triton_result[2]:.3f} GB/s\")\n",
    "\n",
    "# Run the benchmark for CUDA (Torch) for comparison\n",
    "# print(\"\\nBenchmarking fused add-multiply operation with CUDA (PyTorch):\")\n",
    "# torch_result = benchmark_fused(size=1024*1024, provider='torch', scalar=2.0)\n",
    "# print(f\"CUDA (Torch) - Average: {torch_result[0]:.3f} GB/s, Max: {torch_result[1]:.3f} GB/s, Min: {torch_result[2]:.3f} GB/s\")\n",
    "\n",
    "\n",
    "# Define the block sizes and placeholders for the benchmark results to visualize\n",
    "block_sizes = [128, 256, 512, 1024]\n",
    "triton_results = []\n",
    "cuda_result = None\n",
    "\n",
    "# Run benchmarks and store results for Triton at different block sizes\n",
    "for block_size in block_sizes:\n",
    "    triton_result = benchmark_fused(size=1024*1024, provider='triton', scalar=2.0, BLOCK_SIZE=block_size)\n",
    "    triton_results.append(triton_result[0])  # Store average GB/s for each block size\n",
    "\n",
    "# Run the benchmark for CUDA and store the average GB/s result\n",
    "cuda_result = benchmark_fused(size=1024*1024, provider='torch', scalar=2.0)[0]\n",
    "\n",
    "# Append CUDA result to the Triton results for consistent plotting\n",
    "all_results = triton_results + [cuda_result]\n",
    "all_block_sizes = [str(bs) for bs in block_sizes] + [\"CUDA\"]\n",
    "\n",
    "# Adjust y-axis limit to avoid overlapping of values with the figure boundary\n",
    "y_limit = max(all_results) * 1.1  # Set y-axis limit 10% above the maximum value for better readability\n",
    "\n",
    "# Plotting the GB/s values for each block size using Triton and CUDA as a separate bar\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(all_block_sizes, all_results, color=['teal'] * len(triton_results) + ['darkorange'], width=0.5)\n",
    "\n",
    "# Adding labels and title with clarified wording\n",
    "plt.xlabel(\"Configuration\", fontsize=14)\n",
    "plt.ylabel(\"Average Throughput (GB/s)\", fontsize=14)\n",
    "plt.title(\"Fused Add-Multiply Operation: Triton Block Sizes vs. CUDA (Torch)\", fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.ylim(0, y_limit)\n",
    "\n",
    "# Annotate GB/s values on the bars for both Triton and CUDA\n",
    "for i, v in enumerate(all_results):\n",
    "    plt.text(i, v + y_limit * 0.02, f\"{v:.1f} GB/s\", ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "1c2dfa73e83ad571",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtriton\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtriton\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlanguage\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtl\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Set a fixed random seed for reproducibility\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'triton'"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Key Takeaways from Triton Block Size Tuning\n",
    "\n",
    "In our benchmarking, the fused add-multiply operation achieved the strongest performance with `BLOCK_SIZE = 128`, resulting in an average throughput of 233.4 GB/s. Here are some insights into why this block size produced optimal performance, as well as broader conclusions on the benefits of using Triton for custom GPU operations:\n",
    "\n",
    "1. **Efficient Use of Shared Memory**:\n",
    "   - GPUs have a limited amount of shared memory per block. `BLOCK_SIZE = 128` allows optimal utilization of this shared memory without excessive demand, avoiding memory contention or spilling over to slower global memory.\n",
    "   - With smaller block sizes, memory access patterns are more coalesced, which maximizes memory bandwidth efficiency and reduces latency.\n",
    "\n",
    "2. **Increased Occupancy with Smaller Blocks**:\n",
    "   - Occupancy, or the ratio of active warps (groups of threads) to the maximum supported by the hardware, is often higher with smaller block sizes. Higher occupancy keeps more threads active, hiding latency and boosting performance.\n",
    "   - A `BLOCK_SIZE` of 128 may allow more blocks to run concurrently on each Streaming Multiprocessor (SM), ensuring better overall GPU utilization.\n",
    "\n",
    "3. **Better Thread Scheduling and Resource Utilization**:\n",
    "   - Smaller blocks, like 128, align more effectively with the GPU’s scheduling mechanisms, enabling more efficient resource use across cores. This reduces waiting times and enhances parallel efficiency.\n",
    "   - This alignment allows the GPU to swap between active warps efficiently, hiding memory access latency.\n",
    "\n",
    "4. **Improved Cache Efficiency**:\n",
    "   - With `BLOCK_SIZE = 128`, memory accesses align well with cache line sizes, allowing for efficient caching of frequently accessed data.\n",
    "   - Larger block sizes may cause cache thrashing or introduce memory bank conflicts, which increase memory contention. By using smaller blocks, we reduce conflicts and spread memory accesses more evenly.\n",
    "\n",
    "### Triton vs. CUDA (Torch): Performance with Fused Operations\n",
    "\n",
    "Our tests highlighted that while Triton and CUDA (Torch) perform similarly for a simple vector addition, Triton significantly outperforms CUDA when we use a fused add-multiply operation:\n",
    "\n",
    "- **Kernel Fusion Advantage**: The fused add-multiply kernel in Triton consolidates memory accesses, reducing the number of reads/writes to global memory, which boosts performance.\n",
    "- **Block Size Flexibility**: Triton allows precise tuning of parameters like BLOCK_SIZE, giving users more control over the kernel’s performance characteristics. This flexibility enables Triton to fully exploit hardware resources for more complex operations.\n",
    "\n",
    "### Summary \n",
    "\n",
    "The results demonstrate that **choosing the right block size and leveraging Triton’s fusion capabilities are essential for maximizing GPU performance**. The optimal `BLOCK_SIZE = 128` provides an effective balance between high occupancy, optimal memory access, and efficient use of GPU resources. Meanwhile, Triton’s flexibility with fused operations offers clear advantages over CUDA (Torch), particularly for complex workflows that benefit from reduced memory access times.\n",
    "\n",
    "In summary, **Triton’s custom kernel capabilities and flexible parameter tuning make it an ideal choice for performance-sensitive GPU applications**, especially for complex operations where traditional CUDA might not fully utilize the hardware’s potential."
   ],
   "id": "91febe453251529"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c206491304c4f7ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
